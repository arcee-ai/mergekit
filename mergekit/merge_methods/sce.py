# Copyright (C) 2024 Charles O. Goddard
#
# This software is free software: you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This software is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this program. If not, see http://www.gnu.org/licenses/.

import logging
from typing import Any, Dict, List, Optional, Tuple

import torch
from pydantic import BaseModel

from mergekit.architecture import WeightInfo
from mergekit.common import ImmutableMap, ModelReference
from mergekit.graph import Task
from mergekit.merge_methods.base import (
    ConfigParameterDef,
    MergeMethod,
    MergeTensorInput,
)


class SCEMerge(MergeMethod, BaseModel, frozen=True):
    def parameters(self) -> List[ConfigParameterDef]:
        return [
            ConfigParameterDef(name="int8_mask", required=False, default_value=False),
            ConfigParameterDef(name="select_topk", required=False, default_value=1.0),
        ]

    def make_task(
        self,
        output_weight: WeightInfo,
        tensors: MergeTensorInput,
        base_model: Optional[ModelReference],
        parameters: ImmutableMap[str, Any],
        **_kwargs,
    ) -> Task:
        return SCETask(
            tensors=tensors,
            base_model=base_model,
            int8_mask=parameters["int8_mask"],
            select_topk=parameters["select_topk"],
            weight_info=output_weight,
        )


class SCETask(Task[torch.Tensor]):
    tensors: MergeTensorInput
    base_model: ModelReference
    weight_info: WeightInfo
    int8_mask: bool
    select_topk: float

    def uses_accelerator(self) -> bool:
        return True

    def arguments(self) -> Dict[str, Task]:
        return {"tensors": self.tensors}

    def execute(
        self,
        tensors: Dict[ModelReference, torch.Tensor],
        **_kwargs,
    ) -> torch.Tensor:
        # collect task vectors
        tvs, base = get_task_vectors(self.weight_info, self.base_model, tensors)
        if not tvs:
            return base

        deltas = torch.stack([tv["delta"] for tv in tvs], dim=0)
        mask_dtype = torch.int8 if self.int8_mask else base.dtype

        # Select the top Ï„% elements with high variance
        if self.select_topk < 1:
            mask = get_sce_mask(deltas, self.select_topk, mask_dtype)
            mask_expanded = mask.unsqueeze(0).expand_as(deltas)
            deltas = deltas * mask_expanded

        # Calculate matrix level merging coefficient
        weights = get_sce_weight(deltas)
        weights = torch.tensor(weights, dtype=deltas.dtype, device=deltas.device)
        while len(deltas.shape) > len(weights.shape):
            weights.unsqueeze_(-1)

        # Erase elements with minority directions
        erase_mask = get_erase_mask(
            deltas,
            mask_dtype=mask_dtype,
        )
        erased_weights = weights * erase_mask
        mixed_delta = (deltas * erased_weights).sum(dim=0)

        # Normalize
        divisor = (erased_weights).sum(dim=0)
        divisor[divisor == 0] = 1
        mixed_delta /= divisor

        return (base + mixed_delta).to(base.dtype)


def get_task_vectors(
    weight_info: WeightInfo,
    base_model: ModelReference,
    tensors: ImmutableMap[ModelReference, torch.Tensor],
) -> Tuple[List[Dict[str, Any]], torch.Tensor]:
    keys = list(tensors.keys())
    base = tensors[base_model]

    parameter_name = weight_info.name

    res = []
    for model in keys:
        if model == base_model:
            continue

        x = tensors[model].to(base.dtype)
        if x.shape != base.shape:
            if weight_info.is_embed:
                x = x[: base.shape[0], : base.shape[1]]
                logging.warning(f"Using submatrix of {model}:{parameter_name}")
            else:
                logging.warning(
                    f"skipping {model}:{parameter_name} due to size mismatch"
                )
                continue

        delta = x - base
        del x
        del tensors[model]

        d = {}
        d["model"] = model
        d["delta"] = delta
        res.append(d)
    return res, base


def get_erase_mask(
    delta: torch.Tensor,
    mask_dtype: Optional[torch.dtype] = None,
):
    """Returns a mask determining which delta vectors should be merged
    into the final model.
    """
    if mask_dtype is None:
        mask_dtype = delta.dtype

    sign = delta.sign().to(mask_dtype)

    sign_weight = delta.sum(dim=0)
    majority_sign = (sign_weight >= 0).to(mask_dtype) * 2 - 1
    del sign_weight

    return sign == majority_sign


def get_sce_mask(
    deltas: torch.Tensor,
    density: float,
    mask_dtype: Optional[torch.dtype] = None,
):
    if mask_dtype is None:
        mask_dtype = deltas.dtype
    # Calculate variance along the first dimension
    variance = torch.var(deltas, dim=0, unbiased=False)
    # Count non-zero positions in variance
    non_zero_positions_count = torch.count_nonzero(variance)
    # Calculate the number of top elements to select
    k = int(abs(density) * non_zero_positions_count)
    mask = torch.zeros_like(variance, dtype=mask_dtype)
    if k == 0:
        return mask
    assert k > 0, "not gonna zero out the whole tensor buddy"

    # Get the indices of the top k elements with the highest absolute variance
    topk_indices = torch.topk(variance.abs().view(-1), k=k, largest=True).indices

    mask.view(-1)[topk_indices] = 1
    return mask


def get_sce_weight(deltas):
    # Calculate the squared sum of each delta and normalize by the number of elements
    weights = [torch.sum(delta**2).item() / delta.numel() for delta in deltas]

    # Normalize the weights
    sum_weights = sum(weights)
    if sum_weights == 0:
        weights = [1.0 / len(weights)] * len(weights)
    else:
        weights = [w / sum_weights for w in weights]

    return weights
